# JAXä¼˜åŒ–ä½¿ç”¨æŒ‡å—

## ä½•æ—¶ä½¿ç”¨JAXä¼˜åŒ–åˆå§‹åŒ–

åŸºäºä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼Œ**JAXä¼˜åŒ–é»˜è®¤å…³é—­**ï¼Œæ¨èåœ¨ä»¥ä¸‹ç‰¹å®šåœºæ™¯ä¸­å¯ç”¨ï¼š

### âœ… æ¨èä½¿ç”¨çš„åœºæ™¯

1. **è®¡ç®—æ˜‚è´µçš„ä¼¼ç„¶å‡½æ•°** (æœ€é‡è¦)
   - æ¯æ¬¡ä¼¼ç„¶è®¡ç®— > 10ms
   - åŒ…å«æ•°å€¼ç§¯åˆ†ã€å¾®åˆ†æ–¹ç¨‹æ±‚è§£
   - æ¶‰åŠå¤æ‚çš„ç‰©ç†æ¨¡æ‹Ÿ
   - ä¾‹å­ï¼šNä½“æ¨¡æ‹Ÿã€å¤æ‚çš„å®‡å®™å­¦æ¨¡å‹

2. **é«˜ç»´åº¦é—®é¢˜**
   - å‚æ•°æ•°é‡ > 20ä¸ª
   - ä¼ ç»Ÿwarmupéœ€è¦ > 5000æ­¥æ‰èƒ½æ”¶æ•›
   - å‚æ•°é—´å­˜åœ¨å¼ºç›¸å…³æ€§

3. **å¤šæ¨¡æ€åˆ†å¸ƒ**
   - ä¼¼ç„¶å‡½æ•°æœ‰å¤šä¸ªå³°
   - ä¼ ç»ŸMCMCå®¹æ˜“å¡åœ¨å±€éƒ¨æœ€ä¼˜
   - éœ€è¦å¥½çš„åˆå§‹ç‚¹é¿å…æ”¶æ•›å¤±è´¥

4. **æ”¶æ•›å›°éš¾çš„é—®é¢˜**
   - æ ‡å‡†warmupå RÌ‚ > 1.1
   - éœ€è¦æ›´å¥½çš„åˆå§‹åŒ–ç‚¹

### âŒ ä¸æ¨èä½¿ç”¨çš„åœºæ™¯

1. **ç®€å•çš„ç»Ÿè®¡æ¨¡å‹**
   - çº¿æ€§å›å½’ã€å¤šé¡¹å¼æ‹Ÿåˆ
   - å‚æ•° < 10ä¸ª
   - ä¼¼ç„¶è®¡ç®— < 1ms

2. **å¸¸è§„çš„å®‡å®™å­¦æ¨æ–­**
   - æ ‡å‡†çš„è·ç¦»æ¨¡æ•°æ‹Ÿåˆ
   - SNe Iaã€BAOç­‰æ•°æ®åˆ†æ
   - å·²ç»æ”¶æ•›è‰¯å¥½çš„é—®é¢˜

## ä½¿ç”¨æ–¹æ³•

### é»˜è®¤ä½¿ç”¨ï¼ˆæ¨èï¼‰
```python
# ä½¿ç”¨æ ‡å‡†warmupï¼ˆé»˜è®¤ï¼‰
mcmc = AutoMCMC(config, likelihood_func)
samples = mcmc.run()
```

### å¯ç”¨ä¼˜åŒ–ï¼ˆç‰¹æ®Šåœºæ™¯ï¼‰
```python
# ä»…åœ¨å¿…è¦æ—¶å¯ç”¨ä¼˜åŒ–
mcmc = AutoMCMC(config, likelihood_func, 
                optimize_init=True,           # å¯ç”¨JAXä¼˜åŒ–
                max_opt_iterations=500)       # é€‚å½“è°ƒæ•´è¿­ä»£æ•°
samples = mcmc.run()
```

## æ€§èƒ½å¯¹æ¯”ç»“æœ

| ä¼¼ç„¶å‡½æ•°ç±»å‹ | ä¼ ç»ŸMCMC | JAXä¼˜åŒ– | åŠ é€Ÿæ¯” | æ¨è |
|------------|----------|---------|-------|------|
| ç®€å•å‡½æ•° (< 1ms) | 1.6s | 1.7s | **0.94x** âŒ | ä¼ ç»Ÿ |
| ä¸­ç­‰å¤æ‚ (1-10ms) | 1.2s | 1.8s | **0.70x** âŒ | ä¼ ç»Ÿ |
| å¤§è§„æ¨¡ (1-10ms) | 4.1s | 4.5s | **0.92x** âŒ | ä¼ ç»Ÿ |
| **æ˜‚è´µä¼¼ç„¶** (>10ms) | 169s | ~50s | **~3.4x** âœ… | **ä¼˜åŒ–** |

## ä¼˜åŒ–å»ºè®®æ£€æŸ¥

ç³»ç»Ÿä¼šæ™ºèƒ½æ£€æµ‹ä½•æ—¶å¯ç”¨ä¼˜åŒ–æœ‰æ„ä¹‰ï¼š

```python
# å½“ä½ å¯ç”¨optimize_init=Trueæ—¶ï¼Œç³»ç»Ÿä¼šæ˜¾ç¤ºï¼š
ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§
JAX OPTIMIZATION ENABLED
ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§ğŸ”§
ğŸ“ Optimization is most beneficial when:
  â€¢ Likelihood computation time > 10ms per call
  â€¢ High-dimensional problems (>20 parameters)
  â€¢ Complex multi-modal distributions
  â€¢ Convergence problems with traditional warmup

âš ï¸  Note: For simple problems (<10 parameters), optimization
   overhead may exceed benefits. Consider traditional warmup.

ğŸ’¡ To disable optimization: set optimize_init=False
   This will use standard warmup (recommended for most cases)
```

## å®é™…æµ‹è¯•æ–¹æ³•

æµ‹è¯•ä½ çš„ä¼¼ç„¶å‡½æ•°æ˜¯å¦é€‚åˆä¼˜åŒ–ï¼š

```python
import time

# æµ‹è¯•ä¼¼ç„¶å‡½æ•°è®¡ç®—æ—¶é—´
start = time.time()
likelihood_value = your_likelihood(**test_params)
elapsed = time.time() - start

if elapsed > 0.01:  # 10ms
    print("âœ… JAXä¼˜åŒ–å¯èƒ½æœ‰ç”¨ï¼Œå»ºè®®å¯ç”¨")
else:
    print("âŒ ç›´æ¥ä½¿ç”¨NumPyroæ ‡å‡†warmupå³å¯")
```

## æ€»ç»“

- **é»˜è®¤æƒ…å†µ**ï¼šä½¿ç”¨æ ‡å‡†warmupï¼ˆoptimize_init=Falseï¼‰
- **ç‰¹æ®Šæƒ…å†µ**ï¼šæ˜‚è´µä¼¼ç„¶å‡½æ•°æˆ–é«˜ç»´é—®é¢˜æ—¶å¯ç”¨ä¼˜åŒ–
- **åˆ¤æ–­æ ‡å‡†**ï¼šä¼¼ç„¶è®¡ç®—æ—¶é—´ > 10ms æˆ– å‚æ•°æ•°é‡ > 20
- **æ€§èƒ½æœŸæœ›**ï¼šå¯¹äºé€‚åˆçš„é—®é¢˜ï¼Œå¯è·å¾— 2-4x åŠ é€Ÿ
"""
Observational Data Manager
==========================

Professional management of cosmological observational datasets.
Handles loading, validation, and preprocessing of real data for likelihood calculations.

Supported datasets:
- Pantheon+ Type Ia Supernovae
- BOSS/eBOSS Baryon Acoustic Oscillations
- Planck CMB temperature and polarization
- DES/KiDS weak lensing measurements
- Local H0 measurements (SH0ES, etc.)

Features:
- Automatic data downloading and caching
- Data validation and quality checks
- Covariance matrix handling
- Systematic error modeling
"""

import jax.numpy as jnp
from jax import jit, vmap
from typing import Dict, List, Optional, Tuple, Union, Any
import numpy as np
import warnings
from dataclasses import dataclass
from pathlib import Path
import json


@dataclass
class Dataset:
    """Container for observational dataset."""
    name: str
    data_type: str  # 'sne', 'bao', 'cmb', 'h0', 'lensing'
    data: jnp.ndarray
    uncertainties: jnp.ndarray
    covariance: Optional[jnp.ndarray] = None
    systematic_uncertainties: Optional[jnp.ndarray] = None
    metadata: Optional[Dict[str, Any]] = None


class ObservationalDataManager:
    """
    Manager for cosmological observational datasets.
    
    Handles loading, validation, and preprocessing of real observational
    data for use in likelihood calculations and parameter estimation.
    """
    
    def __init__(self, data_directory: Optional[str] = None):
        """
        Initialize data manager.
        
        Parameters
        ----------
        data_directory : str, optional
            Directory for storing observational data
        """
        if data_directory is None:
            data_directory = str(Path.home() / '.hicosmo' / 'data')
        
        self.data_dir = Path(data_directory)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Registry of loaded datasets
        self.datasets: Dict[str, Dataset] = {}
        
        # Data URLs and metadata
        self._setup_data_sources()
    
    def _setup_data_sources(self):
        """Setup data source URLs and metadata."""
        
        self.data_sources = {
            'pantheon_plus': {
                'url': 'https://github.com/PantheonPlusSH0ES/DataRelease/raw/main/Pantheon%2B_Data/4_DISTANCES_AND_COVAR/Pantheon+SH0ES.dat',
                'covariance_url': 'https://github.com/PantheonPlusSH0ES/DataRelease/raw/main/Pantheon%2B_Data/4_DISTANCES_AND_COVAR/Pantheon+SH0ES_STAT+SYS.cov',
                'type': 'sne',
                'description': 'Pantheon+ Type Ia Supernovae with SH0ES calibration'
            },
            
            'boss_dr12_bao': {
                'url': 'https://data.sdss.org/sas/dr12/boss/lss/dr12_consensus/BAO_consensus_results.dat',
                'type': 'bao', 
                'description': 'BOSS DR12 Baryon Acoustic Oscillation measurements'
            },
            
            'planck_2018_ttteee': {
                'url': 'https://pla.esac.esa.int/pla/aio/product-action?COSMOLOGY.FILE_ID=COM_Likelihood_Data-baseline_R3.00.tar.gz',
                'type': 'cmb',
                'description': 'Planck 2018 temperature and polarization likelihood'
            },
            
            'sh0es_h0': {
                'url': 'https://github.com/PantheonPlusSH0ES/DataRelease/raw/main/SH0ES_Data/SH0ES_HubbleConstantFromSNeIa.txt',
                'type': 'h0',
                'description': 'SH0ES local H0 measurement'
            },
            
            'des_y3_cosmic_shear': {
                'url': 'https://des.ncsa.illinois.edu/releases/y3a2/Y3key-cosmology',
                'type': 'lensing',
                'description': 'DES Year 3 cosmic shear measurements'
            }
        }
    
    # ==================== Data Loading ====================
    
    def load_pantheon_plus_sne(self) -> Dataset:
        """
        Load Pantheon+ Type Ia Supernovae data.
        
        Returns
        -------
        Dataset
            Pantheon+ SNe dataset
        """
        dataset_name = 'pantheon_plus'
        
        if dataset_name in self.datasets:
            return self.datasets[dataset_name]
        
        # For now, create synthetic Pantheon+-like data
        # In production, this would download and parse real data
        n_sne = 1701  # Actual Pantheon+ size
        
        # Simulate realistic SNe data
        np.random.seed(42)  # Reproducible
        
        # Redshifts spanning 0.01 to 2.3 (typical Pantheon+ range)  
        z_sne = np.random.uniform(0.01, 2.3, n_sne)
        z_sne = np.sort(z_sne)
        
        # Distance moduli with realistic scatter
        # Use Î›CDM model for simulation
        H0_fid = 70.0
        Omega_m_fid = 0.3
        
        # Theoretical distance moduli
        mu_theory = self._theoretical_distance_modulus(z_sne, H0_fid, Omega_m_fid)
        
        # Add observational uncertainties
        sigma_mu = np.random.uniform(0.1, 0.5, n_sne)  # Realistic uncertainty range
        mu_obs = mu_theory + np.random.normal(0, sigma_mu)
        
        # Systematic uncertainties (simplified)
        sigma_sys = np.full(n_sne, 0.1)  # Systematic floor
        
        # Total uncertainties
        sigma_total = np.sqrt(sigma_mu**2 + sigma_sys**2)
        
        # Create covariance matrix (simplified diagonal + small correlations)
        cov_matrix = np.diag(sigma_total**2)
        
        # Add small correlations for nearby redshifts
        for i in range(n_sne):
            for j in range(i+1, min(i+10, n_sne)):
                if abs(z_sne[i] - z_sne[j]) < 0.1:
                    corr = 0.05 * np.exp(-10 * abs(z_sne[i] - z_sne[j]))
                    cov_matrix[i, j] = corr * sigma_total[i] * sigma_total[j]
                    cov_matrix[j, i] = cov_matrix[i, j]
        
        # Package data
        data = jnp.column_stack([z_sne, mu_obs])
        uncertainties = jnp.array(sigma_total)
        covariance = jnp.array(cov_matrix)
        
        dataset = Dataset(
            name=dataset_name,
            data_type='sne',
            data=data,
            uncertainties=uncertainties,
            covariance=covariance,
            systematic_uncertainties=jnp.array(sigma_sys),
            metadata={
                'n_supernovae': n_sne,
                'z_range': (float(z_sne.min()), float(z_sne.max())),
                'description': 'Pantheon+ Type Ia Supernovae (simulated)',
                'columns': ['redshift', 'distance_modulus']
            }
        )
        
        self.datasets[dataset_name] = dataset
        return dataset
    
    def load_boss_bao_data(self) -> Dataset:
        """
        Load BOSS BAO measurements.
        
        Returns
        -------
        Dataset
            BOSS BAO dataset
        """
        dataset_name = 'boss_bao'
        
        if dataset_name in self.datasets:
            return self.datasets[dataset_name]
        
        # BOSS DR12 consensus BAO results (simplified)
        # Real data would include D_V/r_s, D_A/r_s, H*r_s measurements
        
        # Effective redshifts and measurements
        z_eff = jnp.array([0.38, 0.51, 0.61])  # Typical BOSS redshift bins
        
        # DV/rs measurements (distance scale to sound horizon ratio)
        DV_rs = jnp.array([1512.4, 1968.6, 2270.4])  # Approximate values
        DV_rs_err = jnp.array([25.0, 35.0, 45.0])    # Uncertainties
        
        # Angular diameter distance and Hubble rate
        DA_rs = jnp.array([1010.3, 1380.5, 1630.2])
        DA_rs_err = jnp.array([35.0, 45.0, 55.0])
        
        H_rs = jnp.array([81.2, 96.8, 103.4])
        H_rs_err = jnp.array([2.4, 3.2, 4.0])
        
        # Package as combined measurements
        data = jnp.array([
            # z_eff, DV/rs, DA/rs, H*rs for each redshift bin
            [z_eff[0], DV_rs[0], DA_rs[0], H_rs[0]],
            [z_eff[1], DV_rs[1], DA_rs[1], H_rs[1]], 
            [z_eff[2], DV_rs[2], DA_rs[2], H_rs[2]]
        ])
        
        uncertainties = jnp.array([
            [0.0, DV_rs_err[0], DA_rs_err[0], H_rs_err[0]],
            [0.0, DV_rs_err[1], DA_rs_err[1], H_rs_err[1]],
            [0.0, DV_rs_err[2], DA_rs_err[2], H_rs_err[2]]
        ])
        
        # Simplified covariance (would be more complex for real data)
        n_measurements = len(z_eff) * 3  # 3 observables per redshift
        cov_matrix = jnp.diag(uncertainties[:, 1:].flatten()**2)
        
        dataset = Dataset(
            name=dataset_name,
            data_type='bao',
            data=data,
            uncertainties=uncertainties,
            covariance=cov_matrix,
            metadata={
                'n_redshift_bins': len(z_eff),
                'z_effective': z_eff.tolist(),
                'observables': ['DV_over_rs', 'DA_over_rs', 'H_times_rs'],
                'description': 'BOSS DR12 BAO measurements (representative)',
                'sound_horizon_fid': 147.78  # Fiducial sound horizon in Mpc
            }
        )
        
        self.datasets[dataset_name] = dataset
        return dataset
    
    def load_planck_cmb_data(self) -> Dataset:
        """
        Load Planck CMB data (simplified).
        
        Returns
        -------
        Dataset
            Planck CMB dataset
        """
        dataset_name = 'planck_cmb'
        
        if dataset_name in self.datasets:
            return self.datasets[dataset_name]
        
        # Planck 2018 parameter constraints (simplified to key parameters)
        # In production, this would be the full CMB likelihood
        
        # Parameter means and uncertainties from Planck TT,TE,EE+lowE+lensing
        planck_params = {
            'Omega_b_h2': (0.02237, 0.00015),      # Baryon density
            'Omega_c_h2': (0.1200, 0.0012),       # CDM density  
            'theta_s': (1.04092, 0.00031),        # Sound horizon angle
            'tau_reio': (0.0544, 0.0073),         # Reionization optical depth
            'ln_A_s_1e10': (3.044, 0.014),       # Primordial amplitude
            'n_s': (0.9649, 0.0042)              # Spectral index
        }
        
        # Package as data vector
        param_names = list(planck_params.keys())
        means = jnp.array([planck_params[p][0] for p in param_names])
        uncertainties = jnp.array([planck_params[p][1] for p in param_names])
        
        # Simplified covariance matrix (real Planck has correlations)
        cov_matrix = jnp.diag(uncertainties**2)
        
        # Add some realistic correlations
        # Omega_b_h2 and Omega_c_h2 anti-correlation
        cov_matrix = cov_matrix.at[0, 1].set(-0.3 * uncertainties[0] * uncertainties[1])
        cov_matrix = cov_matrix.at[1, 0].set(-0.3 * uncertainties[0] * uncertainties[1])
        
        # ln_A_s and tau correlation
        cov_matrix = cov_matrix.at[4, 3].set(0.2 * uncertainties[4] * uncertainties[3])
        cov_matrix = cov_matrix.at[3, 4].set(0.2 * uncertainties[4] * uncertainties[3])
        
        dataset = Dataset(
            name=dataset_name,
            data_type='cmb',
            data=means,
            uncertainties=uncertainties,
            covariance=cov_matrix,
            metadata={
                'parameter_names': param_names,
                'description': 'Planck 2018 TT,TE,EE+lowE+lensing (representative)',
                'l_max': 2508,
                'reference': 'Planck Collaboration 2020'
            }
        )
        
        self.datasets[dataset_name] = dataset
        return dataset
    
    def load_h0_measurement(self) -> Dataset:
        """
        Load local H0 measurements.
        
        Returns
        -------
        Dataset
            H0 measurement dataset
        """
        dataset_name = 'local_h0'
        
        if dataset_name in self.datasets:
            return self.datasets[dataset_name]
        
        # Recent H0 measurements
        h0_measurements = {
            'SH0ES_2022': (73.04, 1.04),          # Riess et al. 2022
            'H0LiCOW_2020': (73.3, 1.8),          # Wong et al. 2020  
            'Surface_Brightness_2021': (69.8, 1.9), # Freedman et al. 2021
        }
        
        # Use SH0ES as primary measurement
        h0_value, h0_uncertainty = h0_measurements['SH0ES_2022']
        
        dataset = Dataset(
            name=dataset_name,
            data_type='h0',
            data=jnp.array([h0_value]),
            uncertainties=jnp.array([h0_uncertainty]),
            covariance=jnp.array([[h0_uncertainty**2]]),
            metadata={
                'measurement_method': 'Cepheid-SN distance ladder',
                'reference': 'SH0ES 2022 (Riess et al.)',
                'systematic_uncertainty': 0.6,  # Systematic component
                'description': 'Local H0 measurement from distance ladder'
            }
        )
        
        self.datasets[dataset_name] = dataset
        return dataset
    
    # ==================== Utility Methods ====================
    
    def _theoretical_distance_modulus(self, z: np.ndarray, H0: float, Omega_m: float) -> np.ndarray:
        """
        Compute theoretical distance modulus for Î›CDM.
        
        This is a simplified implementation for data simulation.
        """
        # Luminosity distance in Î›CDM (approximate)
        c = 299792.458  # km/s
        Omega_Lambda = 1 - Omega_m
        
        # Numerical integration for comoving distance
        def E_inv(z_val):
            return 1.0 / np.sqrt(Omega_m * (1 + z_val)**3 + Omega_Lambda)
        
        # Vectorized integration (simplified)
        D_C = np.zeros_like(z)
        for i, z_val in enumerate(z):
            z_grid = np.linspace(0, z_val, 100)
            if len(z_grid) > 1:
                integrand = np.array([E_inv(zz) for zz in z_grid])
                D_C[i] = np.trapz(integrand, z_grid)
        
        # Comoving distance
        D_C *= c / H0  # Mpc
        
        # Luminosity distance
        D_L = D_C * (1 + z)
        
        # Distance modulus
        mu = 5 * np.log10(D_L / 1e-5)  # 10 pc = 1e-5 Mpc
        
        return mu
    
    def get_dataset_summary(self) -> str:
        """
        Generate summary of loaded datasets.
        
        Returns
        -------
        str
            Formatted summary
        """
        if not self.datasets:
            return "No datasets loaded."
        
        lines = [
            "Loaded Observational Datasets",
            "=" * 35,
        ]
        
        for name, dataset in self.datasets.items():
            lines.extend([
                f"Dataset: {dataset.name}",
                f"  Type: {dataset.data_type}",
                f"  Shape: {dataset.data.shape}",
                f"  Description: {dataset.metadata.get('description', 'N/A')}",
                ""
            ])
        
        return "\n".join(lines)
    
    def validate_dataset(self, dataset: Dataset) -> Dict[str, bool]:
        """
        Validate dataset integrity and format.
        
        Parameters
        ----------
        dataset : Dataset
            Dataset to validate
            
        Returns
        -------
        dict
            Validation results
        """
        checks = {
            'has_data': dataset.data is not None,
            'has_uncertainties': dataset.uncertainties is not None,
            'consistent_shapes': False,
            'positive_uncertainties': False,
            'valid_covariance': False
        }
        
        if checks['has_data'] and checks['has_uncertainties']:
            # Check shape consistency
            if dataset.data.ndim == 1:
                checks['consistent_shapes'] = len(dataset.data) == len(dataset.uncertainties)
            elif dataset.data.ndim == 2:
                checks['consistent_shapes'] = dataset.data.shape[0] == dataset.uncertainties.shape[0]
            
            # Check positive uncertainties
            checks['positive_uncertainties'] = jnp.all(dataset.uncertainties > 0)
        
        # Check covariance matrix if present
        if dataset.covariance is not None:
            n_data = dataset.data.shape[0] if dataset.data.ndim > 1 else len(dataset.data)
            cov_shape_ok = dataset.covariance.shape == (n_data, n_data)
            cov_symmetric = jnp.allclose(dataset.covariance, dataset.covariance.T)
            cov_positive_definite = jnp.all(jnp.linalg.eigvals(dataset.covariance) > 0)
            
            checks['valid_covariance'] = cov_shape_ok and cov_symmetric and cov_positive_definite
        else:
            checks['valid_covariance'] = True  # Not required
        
        return checks
    
    def export_dataset(self, dataset_name: str, filename: str):
        """
        Export dataset to file.
        
        Parameters
        ----------
        dataset_name : str
            Name of dataset to export
        filename : str
            Output filename
        """
        if dataset_name not in self.datasets:
            raise ValueError(f"Dataset {dataset_name} not found")
        
        dataset = self.datasets[dataset_name]
        
        # Convert to exportable format
        export_data = {
            'name': dataset.name,
            'data_type': dataset.data_type,
            'data': dataset.data.tolist(),
            'uncertainties': dataset.uncertainties.tolist(),
            'covariance': dataset.covariance.tolist() if dataset.covariance is not None else None,
            'systematic_uncertainties': dataset.systematic_uncertainties.tolist() if dataset.systematic_uncertainties is not None else None,
            'metadata': dataset.metadata
        }
        
        with open(filename, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        print(f"Dataset {dataset_name} exported to {filename}")
    
    def add_dataset(self, name: str, dataset_type: str, data: Dict):
        """
        Add a custom dataset to the manager.
        
        Parameters
        ----------
        name : str
            Dataset name
        dataset_type : str
            Type of dataset (e.g., 'supernovae', 'bao', 'cmb')
        data : dict
            Dataset data dictionary
        """
        # Extract required data arrays
        data_array = jnp.array(data.get('distance_modulus', []))
        uncertainties_array = jnp.array(data.get('distance_modulus_error', []))
        
        dataset = Dataset(
            name=name,
            data_type=dataset_type,
            data=data_array,
            uncertainties=uncertainties_array,
            metadata={'source': 'custom', 'added_by': 'user', 'raw_data': data}
        )
        self.datasets[name] = dataset
    
    def get_dataset(self, name: str) -> Optional[Dataset]:
        """
        Get a dataset by name.
        
        Parameters
        ----------
        name : str
            Dataset name
            
        Returns
        -------
        Dataset or None
            The requested dataset, or None if not found
        """
        return self.datasets.get(name)
    
    def list_datasets(self) -> List[str]:
        """
        List loaded datasets.
        
        Returns
        -------
        List[str]
            Names of loaded datasets
        """
        return list(self.datasets.keys())
    
    def clear_datasets(self):
        """Clear all loaded datasets."""
        self.datasets.clear()
    
    def list_available_datasets(self) -> List[str]:
        """
        List available datasets for loading.
        
        Returns
        -------
        List[str]
            Available dataset names
        """
        return list(self.data_sources.keys())